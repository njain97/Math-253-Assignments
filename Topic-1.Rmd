# Topic 1 Exercises
#Discussion questions
2.4.1, 2.4.3, 2.4.6:

2.4.1:
a) A flexible approach would be better as it would fit the model better and reduce bias. A large sample size would benefit with a flexible model as opposed to an inflexible one. 

b) A flexible approach would be worse as it would overfit the data as the sample size is small. 

c) Flexible would be better as there would be a greater degree of freedom and a flexible model would obtain a better fit. 

d) A flexible model would be worse as flexibility would just increase the amount of variance.

2.4.3:
included picture- called 2.4.3.JPG

b)
squared bias- decreases monotonically because greater flexibility fits the data better. 

Training error - Training error declines monotonically as flexibility increases. This is because as flexibility increases, the data is fit better. 

Test error - concave up as greater flexibility means a better fit till a certain point and then it levels off and starts to increase again (overfits the data). The test error is the sum of Bayes error, bias and variance. Thus it can never lie below the dotted line (Bayes error)

Bayes error - The horizontal dashed line indicates the irreducible error which corresponds to the lowest achievable test MSE among all possible methods. 

Variance - Increases monotonically because increases in flexibility yields overfitting the data. 

2.4.6:

A parametric approach makes an assumption for the functional form or shape of f. It reduces the problem of estimating f down to one of estimating the parameters. Non parametric models do not make an assumption for the functional form or shape of f. It requires a very large number of observations to accurately estimate f. 

The advantages of parametric to regression or classification is that not many observations are needed as compared to a non parametric approach and it simplifies it to just estimating the parameters for a model for f. 

The disadvantages are that the model chosen for f could be wrong which would inaccurately estimate f or overfit the observations if more flexible models are used. 

#Computing Assignment
2.4.8, 2.4.9

2.4.8:
a) 

```{r}
##download.file("http://www-bcf.usc.edu/~gareth/ISL/College.csv", destfile="College.csv")
```

```{r}
auto_file_name = "/home/local/MAC/njain/Math-253-Assignments ssh/College.csv"
```

```{r}
college = read.csv(auto_file_name,header=T,na.strings ="?")
```
b) 

```{r}
#instead of fix, use view(college) in console 
#rownames(college) = college[,1]
#college = college[,-1]
```

c) 

```{r}

#i)
summary(college)

#ii)
pairs(college[,1:10])

#iii) 
plot(college$Private, college$Outstate)

#iv) 
Elite = rep("No", nrow(college))
Elite[college$Top10perc>50] = "Yes"
Elite= as.factor(Elite)
college = data.frame(college, Elite)
summary(college$Elite)
boxplot(college$Elite, college$Outstate)

#v)
par(mfrow=c(2,2))
hist(college$Room.Board)
hist(college$Grad.Rate)
hist(college$Accept, col=3)
hist(college$perc.alumni, col=2)

#vi) 
par(mfrow=c(1,1))
plot(college$Outstate, college$Grad.Rate)

# High tuition correlates to high graduation rate.

plot(college$Elite, college$Grad.Rate)

#Colleges with the most students from top 10% perc (Elite schools) have higher graduation rate. Their box plot is also not as spread out as non-elite schools. 

```

2.4.9: 

```{r}
auto_file_name1 = "/home/local/MAC/njain/Math-253-Assignments ssh/Auto.csv"
Auto = read.csv(auto_file_name1,header=T,na.strings ="?")
Auto= na.omit(Auto)
dim(Auto)
summary(Auto)

#a) 
#Quantitative: mpg, cylinders, displacement, horsepower, weight, acceleration
#Qualitative: year, origin, name

#b)
sapply(Auto[,1:6], range)

#c) 
sapply(Auto[,1:6], mean)
sapply(Auto[,1:6], sd)

#d) 
newAuto = Auto[-(10:85), ]
#dim(newAuto) == dim(Auto) - c(76,0)
#newAuto[9,] == Auto[9,]
#newAuto[10,] == Auto[86,]
sapply(newAuto[,1:6], range)
sapply(newAuto[,1:6], mean)
sapply(newAuto[,1:6], sd)

#e) 
pairs(Auto)

plot(Auto$mpg, Auto$weight)

# Heavier weight correlates with lower mpg.

plot(Auto$mpg, Auto$cylinders)

# There is no relation between cylinders and mpg

plot(Auto$mpg, Auto$year)

#There is no obvious relationship between year and mpg

plot(Auto$horsepower, Auto$acceleration)

#As acceleration increases, horsepower decreases

#f)
#Weight of the car is a good predictor of mpg. It shows a strong negative correlation with mpg. Name, year and cylinder do not have any obvious patterns with mpg and thus would not be good predictors and would overfit the data. 
```
#Theory Assignment 
2.4.2, 2.4.7

2.4.2: 
a) Regression, inference. This is because we are interested in estimating CEO salary based on certain features of the firm. 
n = 500 firms
p = profit, number of employees and industry

b) Classification, prediction. This is because we are trying to classify whether it was a success or failure and predict the result for each product. 
n = 20 similar products previously launched
p = price charged, marketing budget, comp. price, ten other variables

c) Regression, prediction. Trying to find the quantitative output of % change for US dollar. 
n - 52 weeks of 2012 weekly data
p - % change in US market, % change in British market, % change in German market

2.4.7: 

a)
Obs. 1 distance = 3
Obs. 2 distance = 2
Obs. 3 distance = sqrt(10) ~ 3.16
Obs. 4 distance = sqrt(5) ~ 2.236
Obs. 5 distance = sqrt(2) ~ 1.4
Obs. 6 distance = sqrt(3) ~ 1.7 

b) The observation will be green. This is because when K=1, the closest observation is Obs. 5

c) The observation will be Red. When K =3, observations 1, 3 and 5 are closest. 1 and 3 are red while 5 is green. Thus, our prediction would be red. 

d) We would expect K to be small. This is because a small K would be more flexible and fit a non-linear boundary while a large K would try to fit a more linear boundary as it would take many points into consideration. 

