# Topic 4 Exercises: Classification
#Nikhita Jain

Programming Assignment: 4.7.11, 4.7.13

4.7.11:

```{r}
library(ISLR)
library(class)
library(MASS)

attach(Auto)
summary(Auto)

#a) 

mpg01 = rep(0, length(mpg))
mpg01[mpg > median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
```

```{r}
#b) 

cor(Auto[,-9])
#mpg01 is negatively correlated with cylinders, weight, dislacement and horsepower

boxplot(Auto$mpg01, Auto$horsepower)
boxplot(Auto$mpg01, Auto$displacement)
boxplot(Auto$mpg01, Auto$cylinders)
boxplot(Auto$mpg01, Auto$acceleration)
boxplot(Auto$mpg01, Auto$weight)
```

```{r}

#c) 

i <- sample(1:nrow(Auto), size= nrow(Auto)/2)
For_Training <- Auto[i,]
For_Testing <- Auto[-i,]

train = (year %% 2 == 0) # if the year is even
test = !train
Auto.train = Auto[train,]
Auto.test = Auto[test,]
mpg01.test = mpg01[test]

#d)

# LDA
lda.fit = lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = For_Training)
lda.predict = predict(lda.fit, For_Training)
mean(lda.pred$class != mpg01.test)

lda.fit = lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, 
    subset = train)
lda.predict = predict(lda.fit, Auto.test)
mean(lda.pred$class != mpg01.test)

#The test error rate is 12.6% 

#e) 

# QDA
qda.fit = qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, 
    subset = train)
qda.pred = predict(qda.fit, Auto.test)
mean(qda.pred$class != mpg01.test)

#The test error rate is 13.2% 

#f)

# Logistic regression
glm.fit = glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, 
    family = binomial, subset = train)
glm.probs = predict(glm.fit, Auto.test, type = "response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != mpg01.test)

#The test error rate is 12.1%

#g) 

train.X = cbind(cylinders, weight, displacement, horsepower)[train, ]
test.X = cbind(cylinders, weight, displacement, horsepower)[test, ]
train.mpg01 = mpg01[train]
set.seed(1)
# KNN (k=1)
knn.pred = knn(train.X, test.X, train.mpg01, k = 1)
mean(knn.pred != mpg01.test)

# KNN (k=10)
knn.pred = knn(train.X, test.X, train.mpg01, k = 10)
mean(knn.pred != mpg01.test)

# KNN (k=100)
knn.pred = knn(train.X, test.X, train.mpg01, k = 100)
mean(knn.pred != mpg01.test)

#k=1, 15.4% test error rate. k=10, 16.5% test error rate. k=100, 14.3% test error rate. K of 100 seems to perform the best. 100 nearest neighbors.

```
4.7.13

```{r}
library(MASS)
summary(Boston)
attach(Boston)



```


Theory assignment: 4.7.1, 4.7.8, 4.7.9

4.7.1: 
Submitted through a picture - FullSizeRender.jpg

4.7.8:

For KNN with K=1, the training error rate is 0% because for any training observation, its nearest neighbor will be the response itself. Thus KNN has a test error rate of 36%. Since, 36% is greater than 30%, it would be preferable to choose logistic regression for classification of new observations. 

4.7.9:

a) 
p(x) = 0.37(1-p(x))
1.37p(X) = 0.37
p(x) = 0.37/1.37 = 27% 

27% of people on average will actually default on their credit card payment. 

b) 
p(x) = 0.16

0.16 = odds(1-0.16)
odds = 0.16/0.84 = 0.19

The odds that she will default is 0.19 
